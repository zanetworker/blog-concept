---
title: "Quote"
date: 2024-05-04
tags: ["ai","strategy","economics"]
type: quote
sourceEntry: "pre-training-fine-tuning-and-kungfu.md"
sourceEntryTitle: "Pre-training, Fine-tuning, and Kungfu!"
---

> A comparison of training throughput (tokens per second) for the 7B model with a context length of 512 on a p4de.24xlarge node. The lower memory footprint of LoRA allows for substantially larger batch sizes, resulting in an approximate 30% boost in throughput. ~Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2

_Extracted from: pre-training-fine-tuning-and-kungfu.md_
